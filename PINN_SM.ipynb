{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2dSVNYy4dF9mkT5ov+WaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryachavoshi/PINN-SM/blob/main/PINN_SM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import necessary libraries"
      ],
      "metadata": {
        "id": "DGOE13IMh6AW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLpRrNxnh0SY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import RAdam\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn.utils as nn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the training dataset"
      ],
      "metadata": {
        "id": "c0-5BrCjkAtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = pd.read_csv('training.csv')\n",
        "del training_df['Unnamed: 0']"
      ],
      "metadata": {
        "id": "R_b8_nirG_JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model classes. Models: PINN_SM, ANN, Traditional_PINN"
      ],
      "metadata": {
        "id": "zSodqKHru5Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PINN_SM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PINN_SM, self).__init__()\n",
        "        # attention network\n",
        "        self.attention_net = nn.Sequential(\n",
        "            nn.Linear(6, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 10),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        # psi network\n",
        "        self.psi_net = nn.Sequential(\n",
        "            nn.Linear(16, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1)\n",
        "        )\n",
        "        # theta network\n",
        "        self.theta_net = nn.Sequential(\n",
        "            nn.Linear(1, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # K network\n",
        "        self.K_net = nn.Sequential(\n",
        "            nn.Linear(1, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize the weights to be positive using Xavier initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.psi_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        for m in self.theta_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "\n",
        "        for m in self.K_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "        for m in self.attention_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "    # forward pass of the network\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        attention_weights = self.attention_net(inputs[:, 0:6])\n",
        "        new_R = inputs[:, 6:] * attention_weights\n",
        "        inputs = torch.cat([inputs[:, 0:6], new_R], dim=-1)\n",
        "\n",
        "        psi = -torch.exp(self.psi_net(inputs))\n",
        "        K = torch.exp(self.K_net(-torch.log(-psi)))\n",
        "        theta = self.theta_net(-torch.log(-psi))\n",
        "\n",
        "        return psi,K,theta\n",
        "# function defined to keep the theta network and k network positive\n",
        "def enforce_positive_weights(model):\n",
        "    for m in model.theta_net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.abs_()\n",
        "            m.bias.data.abs_()\n",
        "    for m in model.K_net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.abs_()\n",
        "            m.bias.data.abs_()\n",
        "def loss_function(model, inputs, theta):\n",
        "\n",
        "    inputs.requires_grad = True\n",
        "    psi,K,theta_pred = model(inputs)\n",
        "\n",
        "   # Compute gradients with respect to inputs\n",
        "    psi_z = torch.autograd.grad(psi, inputs, grad_outputs=torch.ones_like(psi), create_graph=True)[0][:, 1].unsqueeze(1)\n",
        "    psi_zz = torch.autograd.grad(psi_z, inputs, grad_outputs=torch.ones_like(psi_z), create_graph=True)[0][:, 1].unsqueeze(1)\n",
        "    theta_t = torch.autograd.grad(theta_pred, inputs, grad_outputs=torch.ones_like(theta_pred), create_graph=True)[0][:, 0].unsqueeze(1)\n",
        "    K_z = torch.autograd.grad(K, inputs, grad_outputs=torch.ones_like(K), create_graph=True)[0][:, 1].unsqueeze(1)\n",
        "\n",
        "    # PDE residual for the Richards-Richards equation\n",
        "    r = theta_t - K_z * psi_z - K * psi_zz - K_z\n",
        "\n",
        "    # Loss components\n",
        "    data_fitting_loss = torch.mean(((theta_pred - theta) ** 2))\n",
        "    pde_residual_loss = torch.mean(r ** 2)\n",
        "\n",
        "    # Compute gradients with respect to model parameters\n",
        "    data_fitting_grad = torch.autograd.grad(data_fitting_loss, model.parameters(), create_graph=True,allow_unused=True)\n",
        "    pde_residual_grad = torch.autograd.grad(pde_residual_loss, model.parameters(), create_graph=True,allow_unused=True)\n",
        "\n",
        "    # Calculate maximum gradient of PDE residual component\n",
        "    max_grad_pde_residual = max(g.abs().max() for g in pde_residual_grad if g is not None)\n",
        "\n",
        "    # Calculate average gradient of data-fitting component\n",
        "    avg_grad_data_fitting = torch.mean(torch.stack([g.abs().mean() for g in data_fitting_grad if g is not None]))\n",
        "\n",
        "    # Compute the ratio\n",
        "    gradient_ratio = max_grad_pde_residual / avg_grad_data_fitting\n",
        "\n",
        "    if gradient_ratio>1:\n",
        "\n",
        "      total_loss = data_fitting_loss + (1/gradient_ratio)*pde_residual_loss\n",
        "      #print('PDE RESIDUAL ADJUSTMENT')\n",
        "\n",
        "    else:\n",
        "\n",
        "      total_loss = data_fitting_loss + pde_residual_loss\n",
        "\n",
        "    return total_loss\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        # attention network\n",
        "        self.attention_net = nn.Sequential(\n",
        "            nn.Linear(6, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 10),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        # psi network\n",
        "        self.psi_net = nn.Sequential(\n",
        "            nn.Linear(16, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1)\n",
        "        )\n",
        "        # theta network\n",
        "        self.theta_net = nn.Sequential(\n",
        "            nn.Linear(1, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # K network\n",
        "        self.K_net = nn.Sequential(\n",
        "            nn.Linear(1, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize the weights to be positive using Xavier initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.psi_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        for m in self.theta_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "\n",
        "        for m in self.K_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "        for m in self.attention_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "    # forward pass of the network\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        attention_weights = self.attention_net(inputs[:, 0:6])\n",
        "        new_R = inputs[:, 6:] * attention_weights\n",
        "        inputs = torch.cat([inputs[:, 0:6], new_R], dim=-1)\n",
        "\n",
        "        psi = -torch.exp(self.psi_net(inputs))\n",
        "        K = torch.exp(self.K_net(-torch.log(-psi)))\n",
        "        theta = self.theta_net(-torch.log(-psi))\n",
        "\n",
        "        return psi,K,theta\n",
        "# function defined to keep the theta network and k network positive\n",
        "def enforce_positive_weights(model):\n",
        "    for m in model.theta_net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.abs_()\n",
        "            m.bias.data.abs_()\n",
        "    for m in model.K_net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.abs_()\n",
        "            m.bias.data.abs_()\n",
        "def loss_function(model, inputs, theta):\n",
        "\n",
        "    inputs.requires_grad = True\n",
        "    psi,K,theta_pred = model(inputs)\n",
        "\n",
        "\n",
        "    # Loss components\n",
        "    data_fitting_loss = torch.mean(((theta_pred - theta) ** 2))\n",
        "    total_loss = data_fitting_loss\n",
        "    return total_loss\n",
        "\n",
        "### Traditional PINN\n",
        "\n",
        "class Traditional_PINN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Traditional_PINN, self).__init__()\n",
        "\n",
        "        self.psi_net = nn.Sequential(\n",
        "            nn.Linear(2, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1)\n",
        "        )\n",
        "\n",
        "        self.theta_net = nn.Sequential(\n",
        "            nn.Linear(1, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.K_net = nn.Sequential(\n",
        "            nn.Linear(1, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 90),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(90, 1)\n",
        "        )\n",
        "        # Initialize the weights to be positive using Xavier initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "\n",
        "        for m in self.psi_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        for m in self.theta_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "\n",
        "        for m in self.K_net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data.clamp_(min=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        psi = -torch.exp(self.psi_net(inputs[:,0:2]))\n",
        "        K = torch.exp(self.K_net(-torch.log(-psi)))\n",
        "        theta = self.theta_net(-torch.log(-psi))\n",
        "\n",
        "        return psi,K,theta\n",
        "\n",
        "def enforce_positive_weights(model):\n",
        "    for m in model.theta_net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.abs_()\n",
        "            m.bias.data.abs_()\n",
        "    for m in model.K_net.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.abs_()\n",
        "            m.bias.data.abs_()\n",
        "def loss_function(model, inputs, theta):\n",
        "\n",
        "    inputs.requires_grad = True\n",
        "    psi,K,theta_pred = model(inputs)\n",
        "\n",
        "   # Compute gradients with respect to inputs\n",
        "    psi_z = torch.autograd.grad(psi, inputs, grad_outputs=torch.ones_like(psi), create_graph=True)[0][:, 1].unsqueeze(1)\n",
        "    psi_zz = torch.autograd.grad(psi_z, inputs, grad_outputs=torch.ones_like(psi_z), create_graph=True)[0][:, 1].unsqueeze(1)\n",
        "    theta_t = torch.autograd.grad(theta_pred, inputs, grad_outputs=torch.ones_like(theta_pred), create_graph=True)[0][:, 0].unsqueeze(1)\n",
        "    K_z = torch.autograd.grad(K, inputs, grad_outputs=torch.ones_like(K), create_graph=True)[0][:, 1].unsqueeze(1)\n",
        "\n",
        "    # PDE residual for the Richards-Richards equation\n",
        "    r = theta_t - K_z * psi_z - K * psi_zz - K_z\n",
        "\n",
        "    # Loss components\n",
        "    data_fitting_loss = torch.mean(((theta_pred - theta) ** 2))\n",
        "    pde_residual_loss = torch.mean(r ** 2)\n",
        "\n",
        "    # Compute gradients with respect to model parameters\n",
        "    data_fitting_grad = torch.autograd.grad(data_fitting_loss, model.parameters(), create_graph=True,allow_unused=True)\n",
        "    pde_residual_grad = torch.autograd.grad(pde_residual_loss, model.parameters(), create_graph=True,allow_unused=True)\n",
        "\n",
        "    # Calculate maximum gradient of PDE residual component\n",
        "    max_grad_pde_residual = max(g.abs().max() for g in pde_residual_grad if g is not None)\n",
        "\n",
        "    # Calculate average gradient of data-fitting component\n",
        "    avg_grad_data_fitting = torch.mean(torch.stack([g.abs().mean() for g in data_fitting_grad if g is not None]))\n",
        "\n",
        "    # Compute the ratio\n",
        "    gradient_ratio = max_grad_pde_residual / avg_grad_data_fitting\n",
        "\n",
        "    if gradient_ratio>1:\n",
        "\n",
        "      total_loss = data_fitting_loss + (1/gradient_ratio)*pde_residual_loss\n",
        "      #print('PDE RESIDUAL ADJUSTMENT')\n",
        "\n",
        "    else:\n",
        "\n",
        "      total_loss = data_fitting_loss + pde_residual_loss\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "TnR8IZ6uiXWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training function"
      ],
      "metadata": {
        "id": "6O5edLrcHCGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, dataloader, test_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "        for batch_idx, (inputs_batch, theta_batch) in enumerate(dataloader):\n",
        "            inputs_batch.to(device)\n",
        "            theta_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_function(model, inputs_batch, theta_batch)\n",
        "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
        "                print(f'Warning: Loss is NaN or Inf at epoch {epoch}, batch {batch_idx}')\n",
        "                continue\n",
        "            loss.backward()\n",
        "            nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            enforce_positive_weights(model)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        average_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch}, Average Training Loss: {average_loss}')\n",
        "        test_loss = 0\n",
        "        num_batches_test = 0\n",
        "        for batch_idx, (inputs_batch, theta_batch) in enumerate(test_loader):\n",
        "          inputs_batch.to(device)\n",
        "          theta_batch.to(device)\n",
        "          loss = loss_function(model, inputs_batch, theta_batch)\n",
        "          test_loss += loss.item()\n",
        "          num_batches_test += 1\n",
        "        average_loss = test_loss / num_batches_test\n",
        "        print(f'Epoch {epoch}, Average Test Loss: {average_loss}')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "IWcIhGoIiZrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train - Test split"
      ],
      "metadata": {
        "id": "GsoFdkBQF9ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "input_data = training_df.values[:,0:16]\n",
        "scaler = scaler.fit(input_data)\n",
        "scaled_data = scaler.transform(input_data) #with normalization!\n",
        "inputs = torch.tensor(scaled_data, dtype=torch.float32).to(device)\n",
        "theta = torch.tensor(training_df.iloc[:,16].values, dtype=torch.float32, requires_grad=True).unsqueeze(1).to(device)\n",
        "dataset = TensorDataset(inputs, theta)\n",
        "# Define the split ratio\n",
        "train_size = int(0.6 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "# Create DataLoaders for training and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "qlWJ7H_rieax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 test periods for testing the soil moisture feedback to rainfall"
      ],
      "metadata": {
        "id": "AQ-QrcwjGAth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_ranges = [\n",
        "    ('2024-03-16', '2024-03-18'),\n",
        "    ('2024-04-07', '2024-04-10'),\n",
        "    ('2024-04-30', '2024-05-02')\n",
        "]\n",
        "test1df = training_df[(training_df['time']>'2024-03-16')&(training_df['time']<'2024-03-19')]\n",
        "test2df = training_df[(training_df['time']>'2024-04-07')&(training_df['time']<'2024-04-10')]\n",
        "test3df = training_df[(training_df['time']>'2024-04-30')&(training_df['time']<'2024-05-02')]\n",
        "\n",
        "input_test1 = torch.tensor(scaler.transform(test1df.values[:,0:16]),dtype=torch.float32).to(device)\n",
        "input_test2 = torch.tensor(scaler.transform(test2df.values[:,0:16]),dtype=torch.float32).to(device)\n",
        "input_test3 = torch.tensor(scaler.transform(test3df.values[:,0:16]),dtype=torch.float32).to(device)\n",
        "\n",
        "theta_test1 = torch.tensor(test1df.iloc[:,16].values).to(device)\n",
        "theta_test2 = torch.tensor(test2df.iloc[:,16].values).to(device)\n",
        "theta_test3 = torch.tensor(test3df.iloc[:,16].values).to(device)"
      ],
      "metadata": {
        "id": "qPnsfTRV-zBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ANN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "train(model, optimizer, train_loader, test_loader, epochs=60)"
      ],
      "metadata": {
        "id": "M09Oc2m5iiDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "kbLUyTMtGZnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_true_theta = []\n",
        "all_predicted_theta = []\n",
        "\n",
        "# Iterate over all batches\n",
        "with torch.no_grad():\n",
        "    for batch_id, (input, actual_theta) in enumerate(test_loader):\n",
        "        # Compute true and predicted values\n",
        "        true_theta = actual_theta.cpu().numpy()\n",
        "        psi, K, theta_pred = model(input)\n",
        "        prediction_theta = theta_pred.cpu().numpy()\n",
        "        all_true_theta.append(true_theta)\n",
        "        all_predicted_theta.append(prediction_theta)\n",
        "\n",
        "#all_inputs = np.concatenate(all_inputs, axis=0)\n",
        "all_true_theta = np.concatenate(all_true_theta, axis=0)\n",
        "all_predicted_theta = np.concatenate(all_predicted_theta, axis=0)"
      ],
      "metadata": {
        "id": "bvHBs7GpOk1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = ((all_predicted_theta - all_true_theta)**2).mean()\n",
        "RMSE = MSE**(0.5)\n",
        "print('MSE',MSE)\n",
        "print('RMSE',RMSE)"
      ],
      "metadata": {
        "id": "cNYg6dWuQUUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    true_theta_test1 = theta_test1.cpu().numpy()\n",
        "    psi, K, theta_pred_test1 = model(input_test1)\n",
        "    prediction_theta_test1 = theta_pred_test1.cpu().numpy()\n",
        "\n",
        "plt.plot(true_theta_test1)"
      ],
      "metadata": {
        "id": "omPOMEOhQh-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}